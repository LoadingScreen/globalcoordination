<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Solving Global Coordination | Welcome to the Solving Global Coordination website. This is the description.</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Solving Global Coordination" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Welcome to the Solving Global Coordination website. This is the description." />
<meta property="og:description" content="Welcome to the Solving Global Coordination website. This is the description." />
<link rel="canonical" href="http://localhost:4000/coordination/" />
<meta property="og:url" content="http://localhost:4000/coordination/" />
<meta property="og:site_name" content="Solving Global Coordination" />
<script type="application/ld+json">
{"url":"http://localhost:4000/coordination/","name":"Solving Global Coordination","description":"Welcome to the Solving Global Coordination website. This is the description.","@type":"WebSite","headline":"Solving Global Coordination","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/coordination/assets/css/style.css?v=749f8abc3309138f94f48f39029aa2020ced1429">
    <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="http://localhost:4000/coordination/">Solving Global Coordination</a></h1>
        
        

        <p>Welcome to the Solving Global Coordination website. This is the description.</p>

        
        <p class="view"><a href="https://github.com/LoadingScreen/globalcoordination">View the Project on GitHub <small>LoadingScreen/globalcoordination</small></a></p>
        

        

        
      </header>
      <section>

      <h1>Introduction</h1>
<blockquote cite="https://en.wikipedia.org/wiki/Principia_Discordia">
<p>
  <b>Malaclypse:</b> Everyone is hurting each other, the planet is rampant with injustices, whole societies plunder groups of their own people, mothers imprison sons, children perish while brothers war.
</p>
<p>
  <b>Goddess:</b> What is the matter with that, if it's what you want to do?
</p>
<p>
  <b>Malaclypse:</b> But nobody wants it! Everybody hates it!
</p>
<p>
  <b>Goddess:</b> Oh. Well, then stop.
</p>
</blockquote>
<p>
  The above dialog originates from the quasi-religious text Principia Discordia<sup><a href="#b226:fn:1" class="footnote" id="b226:fn-back:1">1</a></sup>, and was quoted in Scott Alexander's Meditations on Moloch<sup><a href="#b226:fn:2" class="footnote" id="b226:fn-back:2">2</a></sup> — a powerful essay that inspired this document.
</p>
<p>
  In that essay, Scott Alexander points out that a great number of humanity's major ailments — problems like corruption, overfishing, deforestation, global warming, nuclear proliferation (and other arms races), anthropogenic existential risks, and so on — have a common game-theoretic kernel.
</p>
<p>
  That common kernel is a bad Nash equilibrium<sup><a href="#b226:fn:3" class="footnote" id="b226:fn-back:3">3</a></sup> — a game-theoretic situation where players (people and organisations) follow local incentives [local both in space and in time] to a predictably bad global outcome.<sup><a href="#b226:fn:4" class="footnote" id="b226:fn-back:4">4</a></sup> <sup><a href="#b226:fn:5" class="footnote" id="b226:fn-back:5">5</a></sup> In some sense, such problems are all forms of a prisoner's dilemma<sup><a href="#b226:fn:6" class="footnote" id="b226:fn-back:6">6</a></sup> <sup><a href="#b226:fn:7" class="footnote" id="b226:fn-back:7">7</a></sup> where individual players can't unilaterally improve the global outcome. These problems are called collective action problems in social science academia.<sup><a href="#b226:fn:8" class="footnote" id="b226:fn-back:8">8</a></sup> <sup><a href="#b226:fn:9" class="footnote" id="b226:fn-back:9">9</a></sup> Solving them requires some kind of coordination (of varying difficulty) between players.
</p>
<p>
  Is there a way to systematically untangle that evil kernel (“Moloch”) and thus solve many of the problems that plague humanity? Can we just “stop” as the goddess advised Malaclypse to do?
</p>
<p>
  I believe we can.
</p>
<p>
  The key insight here is that in general, said bad Nash equilibria are not deliberately put in place. Rather, they emerge over time for contingent historical reasons (for instance, overfishing is a problem because our fishing technology evolved faster than respective monitoring and enforcement technologies). In other words, there were no “strategic designers” behind bad equilibria with nefarious plans to maintain them.<sup><a href="#b226:fn:10" class="footnote" id="b226:fn-back:10">10</a></sup>
</p>
<br>
  Or, as Michael Vassar<sup><a href="#b226:fn:11" class="footnote" id="b226:fn-back:11">11</a></sup> once eloquently put it:
  <blockquote>The world is like a race car speeding towards a cliff...but the good news is that there's nobody at the wheel!
  </blockquote>
  <br>
  This is not to say that we need some global dictator at the wheel. Instead, let's engineer a “global steering mechanism” that tracks human values and course-corrects the world when those values are undermined!
<br><br><br>
<hr>
  <section>
    <br>
    <h3><b>Next:</b> <a href="/coordination/goalsAndStructure">Goals and Structure</a></h3>
  </section>

<br>

<h2>Footnotes</h2>
<ol class="footnotelist">
<li id="b226:fn:1" class="footnotebody" value="1">
    <b>title:</b> Principia Discordia
    <br />
    <b>url:</b> <a href="https://en.wikipedia.org/wiki/Principia_Discordia" target="_blank">https://en.wikipedia.org/wiki/Principia_Discordia</a>
    <br />
    <b>arcurl:</b> <a href="http://web.archive.org/web/20170922151938/https://en.wikipedia.org/wiki/Principia_Discordia" target="_blank">http://web.archive.org/web/20170922151938/https://en.wikipedia.org/wiki/Principia_Discordia</a>
    <br />
    <b>date:</b> N/A
    <br />
    <b>authors:</b> N/A
    <br />
    <b>summary:</b> The Principia Discordia is a Discordian religious text written by Greg Hill (Malaclypse the Younger) with Kerry Wendell Thornley (Lord Omar Khayyam Ravenhurst). The first edition was printed in 1963. The Principia describes the Discordian Society and its Goddess Eris, as well as the basics of Discordianism. It features typewritten and handwritten text intermixed with clip art, stamps, and seals appropriated from other sources. While the Principia is full of literal contradictions and unusual humor, it contains several passages which suggest that there is serious intent behind the work.
  <a href="#b226:fn-back:1" class="backlink">⏎</a></li>
<li id="b226:fn:2" class="footnotebody" value="2">
  <b>title:</b> Meditations On Moloch
  <br /><b>url:</b> <a href="http://slatestarcodex.com/2014/07/30/meditations-on-moloch/" target="_blank">http://slatestarcodex.com/2014/07/30/meditations-on-moloch/</a>
<br /><b>arcurl:</b> <a href="http://web.archive.org/web/20180414113612/http://slatestarcodex.com/2014/07/30/meditations-on-moloch/" target="_blank">http://web.archive.org/web/20180414113612/http://slatestarcodex.com/2014/07/30/meditations-on-moloch/</a>
<br /><b>date:</b> 2014-07-30
<br /><b>authors:</b> Scott Alexander
<br /><b>summary:</b> Moloch, from the poem by Allen Ginsberg, is representative of the coordination failures that ensure detrimental developments in human society. The author gives many examples of how individual incentives differ from group incentives and how that makes optimal outcomes impossible. Competition forces agents to take the most optimal path to the thing that is being competed over at the expense of other values. There are four reasons “Moloch” hasn't degenerated humanity to subsistence level- excess resources, physical limitations, utility maximization, and coordination. Increasing technology will render all four obsolete. Humanity may end because of a superintelligence optimizing for one thing or be replaced by emulated humanity. This is the result of all the problems Moloch represents. The only way humans can survive is to work toward making sure that when a superintelligent “god” comes into existence it is benevolent toward human values and will help solve our problems instead of discarding us because we're obsolete. The author suggests that we must create one god to kill another- Moloch.
  <a href="#b226:fn-back:2" class="backlink">⏎</a></li>
<li id="b226:fn:3" class="footnotebody" value="3">
    <br /><b>title:</b> Nash equilibrium
    <br /><b>url:</b> <a href="https://en.wikipedia.org/wiki/Nash_equilibrium" target="_blank">https://en.wikipedia.org/wiki/Nash_equilibrium</a>
    <br /><b>arcurl:</b> <a href="http://web.archive.org/web/20180416223743/https://en.wikipedia.org/wiki/Nash_equilibrium" target="_blank">http://web.archive.org/web/20180416223743/https://en.wikipedia.org/wiki/Nash_equilibrium</a>
    <br /><b>date:</b> N/A
    <br /><b>authors:</b> N/A
    <br /><b>summary:</b> A Nash equilibrium is a game state in which no player stands to benefit from changing their course of actions, presuming they are the only one to do so. Perhaps most well-known is the defect-defect state in a prisoner's dilemma — it is not helpful for one of the agents to switch to cooperation while the other remains in defection. Nash equilibria can be found in market competitions, arms races, traffic, and other situations in which multiple agents interact (“games”). Refinements to the math are needed for games of imperfect information and other real-world complications.
  <a href="#b226:fn-back:3" class="backlink">⏎</a></li>
<li id="b226:fn:4" class="footnotebody" value="4">
    <a href="https://www.fhi.ox.ac.uk/wp-content/uploads/Racing-to-the-precipice-a-model-of-artificial-intelligence-development.pdf" target="_blank">https://www.fhi.ox.ac.uk/wp-content/uploads/Racing-to-the-precipice-a-model-of-artificial-intelligence-development.pdf</a>
    <br />
    <b>title:</b> Racing to the Precipice: a Model of Artificial Intelligence Development
    <br /><b>url:</b> <a href="https://www.fhi.ox.ac.uk/wp-content/uploads/Racing-to-the-precipice-a-model-of-artificial-intelligence-development.pdf" target="_blank">https://www.fhi.ox.ac.uk/wp-content/uploads/Racing-to-the-precipice-a-model-of-artificial-intelligence-development.pdf</a>
    <br /><b>arcurl:</b> <a href="http://web.archive.org/web/20180124033107/https://www.fhi.ox.ac.uk/wp-content/uploads/Racing-to-the-precipice-a-model-of-artificial-intelligence-development.pdf" target="_blank">http://web.archive.org/web/20180124033107/https://www.fhi.ox.ac.uk/wp-content/uploads/Racing-to-the-precipice-a-model-of-artificial-intelligence-development.pdf</a>
    <br /><b>date:</b> 2013-10
    <br /><b>authors:</b> Stuart Armstrong, Nick Bostrom, and Carl Shulman
    <br /><b>summary:</b> A simple mathematical model of a technological race is presented: several teams have utility 1 if they win the race without disaster, 0 if they win and cause a disaster, and 1-e when some other team wins (where e is an “enmity level”). Each team can pick a safety level s, which corresponds to the probability of not causing a disaster, and trades off against chance of winning the race. After analyzing Nash equilibria of the game, it is seen that higher enmity and having more teams increase the total risk of disaster. Surprisingly, providing teams with more information about team capabilities (of other teams and themselves) also increases the risk in most cases.
  <a href="#b226:fn-back:4" class="backlink">⏎</a></li>
<li id="b226:fn:5" class="footnotebody" value="5">
    <b>title:</b> Rationalist Explanations for War
    <br /><b>url:</b> <a href="https://web.stanford.edu/group/fearon-research/cgi-bin/wordpress/wp-content/uploads/2013/10/Rationalist-Explanations-for-War.pdf" target="_blank">https://web.stanford.edu/group/fearon-research/cgi-bin/wordpress/wp-content/uploads/2013/10/Rationalist-Explanations-for-War.pdf</a>
    <br /><b>arcurl:</b> <a href="http://web.archive.org/web/20180117162651/http://web.stanford.edu/group/fearon-research/cgi-bin/wordpress/wp-content/uploads/2013/10/Rationalist-Explanations-for-War.pdf" target="_blank">http://web.archive.org/web/20180117162651/http://web.stanford.edu/group/fearon-research/cgi-bin/wordpress/wp-content/uploads/2013/10/Rationalist-Explanations-for-War.pdf</a>
    <br /><b>date:</b> 2013-10-24
    <br /><b>authors:</b> James D. Fearon
    <br /><b>summary:</b> Rationalist explanations for war are those which assume that even while knowing the full cost of war, and taking responsibility for it, engaging in war might still sometimes be a good decision (better than e.g. every possible peaceful agreement). Common explanations of this type fail to account for the possibility of bargaining, or the possibility of gathering more information. However, some appear legitimate: possession of private information about military strength, which cannot be credibly transmitted; commitment problems, which prevent parties from trusting an agreement; and issue indivisibility, which means that on certain issues a compromise is inherently not a satisfying solution.
  <a href="#b226:fn-back:5" class="backlink">⏎</a></li>
<li id="b226:fn:6" class="footnotebody" value="6">
    Or a stag hunt, notably different from the prisoner's dilemma in that cooperation as well as defection is a Nash equilibrium.
    <br />
    <a href="https://en.wikipedia.org/wiki/Stag_hunt" target="_blank">https://en.wikipedia.org/wiki/Stag_hunt</a>
  <a href="#b226:fn-back:6" class="backlink">⏎</a></li>
<li id="b226:fn:7" class="footnotebody" value="7">
    <b>title:</b> Prisoner's dilemma
    <br /><b>url:</b> <a href="https://en.wikipedia.org/wiki/Prisoner%27s_dilemma" target="_blank">https://en.wikipedia.org/wiki/Prisoner%27s_dilemma</a>
    <br /><b>arcurl:</b> <a href="http://web.archive.org/web/20180417233358/https://en.wikipedia.org/wiki/Prisoner's_dilemma" target="_blank">http://web.archive.org/web/20180417233358/https://en.wikipedia.org/wiki/Prisoner's_dilemma</a>
    <br /><b>date:</b> N/A
    <br /><b>authors:</b> N/A
    <br /><b>summary:</b> Prisoner's dilemma is a classic game analysed in game theory. The basic premise is that two members of a gang are arrested, and each prisoner is in solitary confinement with no way to talk to the other. They offer each prisoner a choice: they may either betray their co-conspirator, or stay silent. If both prisoners stay silent, they both go to prison for one year, whereas if they both talk they go to prison for two years. However, if one prisoner stays silent while their partner gives them up, they will go to prison for three years while the person who gave them up goes free. Because of this, it is in each prisoner's best interest to betray their partner, which ironically results in a worse outcome than if both players had acted against their best interests.
  <a href="#b226:fn-back:7" class="backlink">⏎</a></li>
<li id="b226:fn:8" class="footnotebody" value="8">
    <b>title:</b> Collective Action
    <br /><b>url:</b> <a href="https://books.google.ee/books/about/Collective_Action.html?id=9n445z1ZaLMC" target="_blank">https://books.google.ee/books/about/Collective_Action.html?id=9n445z1ZaLMC</a>
    <br /><b>arcurl:</b> N/A
    <br /><b>date:</b> 1982
    <br /><b>authors:</b> Russell Hardin
    <br /><b>summary:</b> Public choice is a discipline which explores how people often make choices in public life which differ from what an economic analysis would predict based on their selfish motives. Russell Hardin brings together various theoretical models and real-world examples to analyse the question.
  <a href="#b226:fn-back:8" class="backlink">⏎</a></li>
<li id="b226:fn:9" class="footnotebody" value="9">
    <b>title:</b> Interview with Russell Hardin
    <br /><b>url:</b> <a href="http://revistaestudospoliticos.com/wp-content/uploads/2014/04/7p30-50.pdf" target="_blank">http://revistaestudospoliticos.com/wp-content/uploads/2014/04/7p30-50.pdf</a>
    <br /><b>arcurl:</b> <a href="http://web.archive.org/web/20170325171645/http://revistaestudospoliticos.com/wp-content/uploads/2014/04/7p30-50.pdf" target="_blank">http://web.archive.org/web/20170325171645/http://revistaestudospoliticos.com/wp-content/uploads/2014/04/7p30-50.pdf</a>
    <br /><b>date:</b> 2013-02
    <br /><b>authors:</b> Cristina Buarque and Fernando Lattman-Weltman
    <br /><b>summary:</b> Russell Hardin shifted his academic focus from physics and mathematics to political science, and is known for his work on collective action. He found that his advanced background in topological set theory made it very easy for him to master game theory. A significant inspiration was the work of Mancur Olson, who brought the “free rider problem” into other fields than economics. Sociologists and political scientists tend to struggle with topics that touch on this problem, and other cases where prisoner's dilemma-like reasoning is applicable. They often use analysis based on contractarianism, but it's the wrong tool. A central concept that can be used to analyse those spheres of public life is acquiescence — all the situations in which people quietly agree to go along with something (such as people who don't vote).
  <a href="#b226:fn-back:9" class="backlink">⏎</a></li>
<li id="b226:fn:10" class="footnotebody" value="10">
    Of course, this is not to say that there aren't powerful people and organisations who happen to benefit from the status quo. identifying them and ensuring their cooperation may be instrumental to fixing respective nash equilibria
  <a href="#b226:fn-back:10" class="backlink">⏎</a></li>
<li id="b226:fn:11" class="footnotebody" value="11">
    <a href="https://en.wikipedia.org/wiki/MetaMed" target="_blank">https://en.wikipedia.org/wiki/MetaMed</a>
  <a href="#b226:fn-back:11" class="backlink">⏎</a></li>
<li id="b226:fn:12" class="footnotebody" value="12">
    Dictators cannot be trusted to serve common good [wiki-good]: regardless of what intentions they start out with, their final values tend to converge towards maintaining power and minimising the probability of getting killed in a coup.
    <br />
    <a href="https://youtu.be/rStL7niR7gs" target="_blank">https://youtu.be/rStL7niR7gs</a>
    <br />
    <a href="https://www.amazon.com/exec/obidos/ASIN/1610391845/" target="_blank">https://www.amazon.com/exec/obidos/ASIN/1610391845/</a>

  <a href="#b226:fn-back:12" class="backlink">⏎</a></li>
</ol>


      </section>
      <footer>
        
        <p>This project is maintained by <a href="https://github.com/LoadingScreen">LoadingScreen</a></p>
        
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="/coordination/assets/js/scale.fix.js"></script>
    
  </body>
</html>
